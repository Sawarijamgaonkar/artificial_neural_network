{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DFNsIZ8Kmkaz"
      },
      "outputs": [],
      "source": [
        "#By: Sawari Jamgaonkar, 22BAI10393\n",
        "import numpy as np\n",
        "\n",
        "class SingleLayerPerceptron:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        # Initialize the perceptron with a learning rate and number of training epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def activation_function(self, x):\n",
        "        # Step activation function: returns 1 if input is >= 0, otherwise returns 0\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Training the perceptron\n",
        "        num_samples, num_features = X.shape\n",
        "        # Initialize weights and bias to zero\n",
        "        self.weights = np.zeros(num_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Training process over a number of epochs\n",
        "        for _ in range(self.epochs):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                # Calculate the linear output\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                # Apply the activation function to get the predicted class\n",
        "                y_pred = self.activation_function(linear_output)\n",
        "\n",
        "                # Update the weights and bias if the prediction is wrong\n",
        "                # Update rule: weight update is proportional to the error\n",
        "                update = self.learning_rate * (y[idx] - y_pred)\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict function to classify input samples\n",
        "        # Compute linear combination and apply activation function for each sample\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return np.array([self.activation_function(x) for x in linear_output])\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Sample data for an AND gate\n",
        "    # Input features\n",
        "    X = np.array([\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]\n",
        "    ])\n",
        "    # Corresponding labels\n",
        "    y = np.array([0, 0, 0, 1])\n",
        "    # Initialize the perceptron with learning rate and epochs\n",
        "    perceptron = SingleLayerPerceptron(learning_rate=0.1, epochs=10)\n",
        "    # Train the perceptron with the training data\n",
        "    perceptron.fit(X, y)\n",
        "     # Make predictions on the training data\n",
        "    predictions = perceptron.predict(X)\n",
        "    print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "165wVWYLppfl",
        "outputId": "ca2031cc-53f6-4b55-a950-6b8c95511887"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 0 0 1]\n"
          ]
        }
      ]
    }
  ]
}