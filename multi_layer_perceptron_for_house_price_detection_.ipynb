{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSWQ5wXIs3ocuK439jSxgP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sawarijamgaonkar/artificial_neural_network/blob/main/multi_layer_perceptron_for_house_price_detection_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4vL7yiuTemi",
        "outputId": "9e1c63f6-30c8-4349-8294-1994cfc241bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000, Loss: 17392503879.9158\n",
            "Epoch 100/1000, Loss: 130614902484281.6875\n",
            "Epoch 200/1000, Loss: 2297574014457.4824\n",
            "Epoch 300/1000, Loss: 40735692741.3815\n",
            "Epoch 400/1000, Loss: 1042540841.2924\n",
            "Epoch 500/1000, Loss: 344419805.0608\n",
            "Epoch 600/1000, Loss: 332141289.5513\n",
            "Epoch 700/1000, Loss: 331925335.6761\n",
            "Epoch 800/1000, Loss: 331921537.4909\n",
            "Epoch 900/1000, Loss: 331921470.6886\n",
            "Sample predictions: [130616.01957806 130616.01957806 130616.01957806 130616.01957806\n",
            " 130616.01957806]\n",
            "Sample actuals: [162949.84582185 150823.38522701 154287.02349224 109942.90900085\n",
            " 113427.38720087]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# multi_layer_perceptron.py\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class MultiLayerPerceptron:\n",
        "    \"\"\"\n",
        "    Multi-layer perceptron for regression problems.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01, epochs=1000):\n",
        "        \"\"\"\n",
        "        Initialize the neural network.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Number of input features.\n",
        "            hidden_dim (int): Number of neurons in the hidden layer.\n",
        "            output_dim (int): Number of output neurons.\n",
        "            learning_rate (float): Learning rate for gradient descent.\n",
        "            epochs (int): Number of training iterations.\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.rand(input_dim, hidden_dim) * 0.01\n",
        "        self.bias_hidden = np.zeros(hidden_dim)\n",
        "        self.weights_hidden_output = np.random.rand(hidden_dim, output_dim) * 0.01\n",
        "        self.bias_output = np.zeros(output_dim)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        \"\"\"ReLU activation function.\"\"\"\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(x):\n",
        "        \"\"\"Derivative of ReLU.\"\"\"\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the output for given input data.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input data (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted output.\n",
        "        \"\"\"\n",
        "        # Forward pass\n",
        "        hidden_layer = self.relu(np.dot(X, self.weights_input_hidden) + self.bias_hidden)\n",
        "        output_layer = np.dot(hidden_layer, self.weights_hidden_output) + self.bias_output\n",
        "        return output_layer\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the MLP using gradient descent.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Training data (n_samples, n_features).\n",
        "            y (np.ndarray): Target values (n_samples, output_dim).\n",
        "        \"\"\"\n",
        "        for epoch in range(self.epochs):\n",
        "            # Forward pass\n",
        "            hidden_layer = self.relu(np.dot(X, self.weights_input_hidden) + self.bias_hidden)\n",
        "            output_layer = np.dot(hidden_layer, self.weights_hidden_output) + self.bias_output\n",
        "\n",
        "            # Compute loss (Mean Squared Error)\n",
        "            loss = np.mean((y - output_layer) ** 2)\n",
        "\n",
        "            # Backward pass\n",
        "            output_error = 2 * (output_layer - y) / y.shape[0]\n",
        "            grad_hidden_output = np.dot(hidden_layer.T, output_error)\n",
        "            grad_bias_output = np.sum(output_error, axis=0)\n",
        "\n",
        "            hidden_error = np.dot(output_error, self.weights_hidden_output.T) * self.relu_derivative(hidden_layer)\n",
        "            grad_input_hidden = np.dot(X.T, hidden_error)\n",
        "            grad_bias_hidden = np.sum(hidden_error, axis=0)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.weights_hidden_output -= self.learning_rate * grad_hidden_output\n",
        "            self.bias_output -= self.learning_rate * grad_bias_output\n",
        "            self.weights_input_hidden -= self.learning_rate * grad_input_hidden\n",
        "            self.bias_hidden -= self.learning_rate * grad_bias_hidden\n",
        "\n",
        "            # Debugging output\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}/{self.epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate synthetic data (house prices based on features)\n",
        "    np.random.seed(42)\n",
        "    X = np.random.rand(500, 3)  # 500 samples, 3 features (e.g., bedrooms, sq ft, city proximity)\n",
        "    y = 100000 + (X[:, 0] * 50000) + (X[:, 1] * 30000) - (X[:, 2] * 20000)\n",
        "    y = y.reshape(-1, 1)  # Reshape for compatibility\n",
        "\n",
        "    # Split into train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Initialize and train the MLP\n",
        "    mlp = MultiLayerPerceptron(input_dim=3, hidden_dim=10, output_dim=1, learning_rate=0.01, epochs=1000)\n",
        "    mlp.train(X_train, y_train)\n",
        "\n",
        "    # Predict on test data\n",
        "    predictions = mlp.predict(X_test)\n",
        "    print(\"Sample predictions:\", predictions[:5].flatten())\n",
        "    print(\"Sample actuals:\", y_test[:5].flatten())\n"
      ]
    }
  ]
}